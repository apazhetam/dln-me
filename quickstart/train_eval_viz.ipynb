{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1975b6f7-8696-4322-ba61-79d7ce6f39af",
   "metadata": {},
   "source": [
    "#### In this notebook, we demonstrate how to use the DLN code to train, evaluate, and visualize models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a189c405-5be8-4578-8552-c391de9a88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to the parent directory\n",
    "import os\n",
    "os.chdir(\"./..\")\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from experiments.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335223a7-c431-4516-aa3f-0c02eeb3418a",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "#### Let's use the dataset we prepared in the prepare_dataset notebook. We present a general use case here. For more advanced functions such as pruning, freezing, and the unified phase, see the descriptions in `experiments/main.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd3a01-8e17-4627-a3d1-6f0eaf25b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/main.py \\\n",
    "--train_model True \\\n",
    "--dataset Heart \\\n",
    "--seed 0 \\\n",
    "--num_epochs 1000 \\\n",
    "--batch_size 64 \\\n",
    "--learning_rate 0.2 \\\n",
    "--tau_out 3 \\\n",
    "--grad_factor 1.2 \\\n",
    "--first_hl_size 50 \\\n",
    "--last_hl_size_wrt_first 0.25 \\\n",
    "--num_hidden_layers 4 \\\n",
    "--num_heads 1 \\\n",
    "--discretize_strategy tree \\\n",
    "--continuous_resolution 4 \\\n",
    "--concat_input True \\\n",
    "--save_model\n",
    "\n",
    "# last_hidden_layer_size = first_hidden_layer_size x last_hl_size_wrt_first\n",
    "# The middle hidden layers will have sizes in a geometric progression from the first to the last layer\n",
    "# Will save the model with the best mean train + val balanced-class accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081517f6-72b5-418f-aea3-c5228e3d6bd2",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "#### We can use the ```--evaluate_model``` flag, which loads the model and evaluates its balanced-class accuracy. It then attempts to simplify the model using SymPy before evaluating the modelâ€™s high-level OPs, basic gate-level OPs, number of parameters, and disk space usage. If simplification is successful, the simplified model is used for these evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831c553-1990-43ce-8d59-3b5a2946c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/main.py \\\n",
    "--train_model False \\\n",
    "--evaluate_model True \\\n",
    "--dataset Heart \\\n",
    "--seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4862e2-c613-44f8-9ab8-499aab3b3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the eval results\n",
    "\n",
    "import json\n",
    "from experiments.utils import *\n",
    "\n",
    "results_path = get_results_path(dataset='Heart', seed=0)\n",
    "with open(f\"{results_path}/eval_results.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d334fd",
   "metadata": {},
   "source": [
    "#### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(head_train_loaders): 1\n",
      "NUM_HEADS: 1\n",
      "----------\n",
      "depth: 7; num layers: 10\n",
      "idx: 0; layer: ThresholdLayer(34, 34), depth: 1, next_layer_idx: 1\n",
      "idx: 1; layer: LogicLayer(34, 50), depth: 2, next_layer_idx: 2\n",
      "idx: 2; layer: LogicLayer(84, 31), depth: 3, next_layer_idx: 3\n",
      "idx: 3; layer: LogicLayer(65, 19), depth: 4, next_layer_idx: 4\n",
      "idx: 4; layer: LogicLayer(53, 12), depth: 5, next_layer_idx: 5\n",
      "idx: 5; layer: SumLayer(\n",
      "  12, 2\n",
      "  (sum_weights): ParameterList(  (0): Parameter containing: [torch.float32 of size 12x2])\n",
      "), depth: 6, next_layer_idx: 6\n",
      "idx: 6; layer: MultiHeadedSumLayer(\n",
      "  12, 2\n",
      "  (sum_weights): ParameterList(  (0): Parameter containing: [torch.float32 of size 12x2])\n",
      "), depth: 7, next_layer_idx: None\n",
      "idx: 7; layer: ThresholdLayer(34, 34), depth: 2, next_layer_idx: 2\n",
      "idx: 8; layer: ThresholdLayer(34, 34), depth: 3, next_layer_idx: 3\n",
      "idx: 9; layer: ThresholdLayer(34, 34), depth: 4, next_layer_idx: 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "head_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "epochs = 4000\n",
    "\n",
    "results_path = get_results_path(dataset='Heart', seed=0)\n",
    "file_path = f\"{results_path}/results_v3_{epochs}.txt\"\n",
    "csv_file_path = file_path.replace(\".txt\", \".csv\")\n",
    "\n",
    "# Prepare the CSV file with a header\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"num_heads\", \"acc_testLoader_eval_mode\"])\n",
    "\n",
    "for val in head_vals:\n",
    "    !python experiments/main.py \\\n",
    "    --train_model True \\\n",
    "    --dataset Heart \\\n",
    "    --seed 0 \\\n",
    "    --num_epochs {epochs} \\\n",
    "    --batch_size 64 \\\n",
    "    --learning_rate 0.2 \\\n",
    "    --tau_out 3 \\\n",
    "    --grad_factor 1.2 \\\n",
    "    --first_hl_size 50 \\\n",
    "    --last_hl_size_wrt_first 0.25 \\\n",
    "    --num_hidden_layers 4 \\\n",
    "    --num_heads {val} \\\n",
    "    --discretize_strategy tree \\\n",
    "    --continuous_resolution 4 \\\n",
    "    --concat_input True \\\n",
    "    --save_model\n",
    "\n",
    "    !python experiments/main.py \\\n",
    "    --train_model False \\\n",
    "    --evaluate_model True \\\n",
    "    --dataset Heart \\\n",
    "    --seed 0\n",
    "\n",
    "    with open(f\"{results_path}/eval_results.json\", 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the accuracy value\n",
    "    acc_eval_mode = data.get(\"acc_testLoader_eval_mode\", None)\n",
    "    if acc_eval_mode is not None:\n",
    "        # Append the num_heads and accuracy to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([val, acc_eval_mode])\n",
    "\n",
    "    results_string = json.dumps(data, indent=4)\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(f\"num_heads = {val}:\\n\" + results_string + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933e3cb-866f-4abe-86e5-53449668cec9",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "#### We use Graphviz to render DLNs generated from SymPy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca69d2a-a5fb-4344-af85-85b2f72fde27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/ash/Projects/apazhetam/dln/experiments/DLN_viz.py\"\u001b[0m, line \u001b[35m189\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    dicts = execute_and_capture_dicts(args.sympy_code_path)\n",
      "  File \u001b[35m\"/Users/ash/Projects/apazhetam/dln/experiments/DLN_viz.py\"\u001b[0m, line \u001b[35m175\u001b[0m, in \u001b[35mexecute_and_capture_dicts\u001b[0m\n",
      "    raise ValueError(\"Script did not produce any output.\")\n",
      "\u001b[1;35mValueError\u001b[0m: \u001b[35mScript did not produce any output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python experiments/DLN_viz.py \\\n",
    "results/Heart/seed_0/sympy_code.py \\\n",
    "quickstart/example/viz\n",
    "\n",
    "# A file named viz.png will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883589b-3a91-4d23-9808-10ceb47448bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"quickstart/example/viz.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513fdb1-02cd-4128-9293-6f0dc92c5951",
   "metadata": {},
   "source": [
    "#### The Heart dataset has 5 continuous and 14 categorical features. The DLN uses only 2 continuous and 11 categorical features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dln-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
